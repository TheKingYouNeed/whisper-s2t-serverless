# RunPod Project Configuration

name = "whisper-s2t-api"

[project]
# uuid                   - Unique identifier for the project. Automatically generated.
#
# volume_mount_path      - Default volume mount path in serverless environment. Changing this may affect data persistence.
#
# base_image             - Base Docker image for the project environment. Includes essential packages and CUDA support.
#                        - Use 'runpod/base' as a starting point. Customize only if you need additional packages or configurations.
#
# gpu_types              - List of GPU types for your development pod. Order the types from most preferred to least preferred.
#                        - The pod uses the first available type from this list.
#                        - For a full list of supported GPU types, visit: https://docs.runpod.io/references/gpu-types
#
# gpu_count              - Number of GPUs to allocate for this pod.
#
# ports                  - Ports to expose and their protocols. Configure as needed for your application.
#
# container_disk_size_gb - Disk space allocated to the container. Adjust according to your needs.

uuid = "3d9071fb"
base_image = "runpod/base:0.6.1-cuda12.1.0"
gpu_types = [
    "NVIDIA GeForce RTX 3090",  # 24GB - CHEAPEST
    "NVIDIA RTX A5000",         # 24GB - CHEAP
    "NVIDIA RTX A4000",         # 16GB
    "NVIDIA RTX A4500",         # 20GB
    "NVIDIA GeForce RTX 4080",  # 16GB
]
gpu_count = 1
# volume_mount_path = "/runpod-volume"  # Disabled - using container disk only
ports = "4040/http, 7270/http, 22/tcp" # FileBrowser, FastAPI, SSH
container_disk_size_gb = 20

[project.env_vars]
# WhisperS2T Configuration
WHISPER_MODEL = "large-v3"
WHISPER_BACKEND = "CTranslate2"
WHISPER_COMPUTE_TYPE = "float16"

# RunPod Configuration
POD_INACTIVITY_TIMEOUT = "60"
RUNPOD_DEBUG_LEVEL = "info"
UVICORN_LOG_LEVEL = "warning"

[endpoint]
# Configure the deployed endpoint.
# For a full list of endpoint configurations, visit: https://docs.runpod.io/serverless/references/endpoint-configurations
#
# active_workers - The minimum number of workers your endpoint has running at any given point.
#                - Setting this amount to 1 or more runs that number of "always on" workers.
#                - These workers respond to job requests without any cold start delay.
#
# max_workers    - The maximum number of workers your endpoint has running at any given point.

active_workers = 0
max_workers = 3
flashboot = true

[runtime]
# python_version    - Python version to use for the project.
#
# handler_path      - Path to the handler file for the project. Adapt example scripts from Hugging Face in this file.
#
# requirements_path - Path to the requirements file for the project. Add dependencies from Hugging Face in this file.

python_version = "3.11"
handler_path = "src/handler.py"
requirements_path = "builder/requirements.txt"
